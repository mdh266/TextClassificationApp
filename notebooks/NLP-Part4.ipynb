{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mdh266/TextClassificationApp/blob/feature%2Fkeras/NLP_Part4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification 4: Deep Learning With Tensorflow and Optuna\n",
    "--------------\n",
    "\n",
    "__[1. Introduction](#first-bullet)__\n",
    "\n",
    "__[2. Vectorizing Text](#second-bullet)__\n",
    "\n",
    "__[3. Handling Imbalance In The Data](#third-bullet)__\n",
    "\n",
    "__[4. Building A Convolutional Neural Network With Keras](#fourth-bullet)__\n",
    "\n",
    "__[5. Hyperparameter Tuning with Optuna](#fifth-bullet)__\n",
    "\n",
    "__[6. Next Steps](#sixth-bullet)__\n",
    "\n",
    "\n",
    "## Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "------------------\n",
    "\n",
    "In this post I want to extend on the last [model](http://michael-harmon.com/blog/NLP2.html) in my blog series on text classification where I used a SVM to predict the topic of papers in arxiv based on their abstract. For reference the topics were \"Machine Learning\", \"Computer Vision\", \"Artifical Intelligence\" and \"Robotics\" and there was imbalance in the classes.\n",
    "\n",
    "In this post I will use a [Convolutional Neural Network (CNN)](https://en.wikipedia.org/wiki/Convolutional_neural_network) model with [Tensorflow](https://www.tensorflow.org/) and [Keras](https://keras.io/) to predict the topic of the abstract and use [Optuna](https://optuna.org/) to optimize the hyperparamters of the model. Keras is a high level library that makes building complex deep learning models relatively easy as well as being built into [Tensorflow](https://www.tensorflow.org/) which is a production ready framework. Optuna is powerful automatic hyperparameter tuning library that uses a *define-by-run* design that makes it elegant and easy to use. I have just started using this library and have been particularly impressed with the design and felt it was extremely intuitve. While CNN's are no longer the state-of-the-art algorithms for text classification, they still perform quite well and I wanted to explore how they would work on this problem. I should note that, the point of this isn't to build the most high performing model (, but rather to show all these tools fit together build a deep learning model end-to-end.\n",
    "\n",
    "\n",
    "Before we get started building the model let's quickly go over vectorizing text as the process I use in this post is different from prior posts which used the [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "train_df = pd.read_json(\"gs://harmon-arxiv/train_abstracts.json\",\n",
    "                        storage_options={\"token\": \"credentials.json\"})\n",
    "    \n",
    "val_df = pd.read_json(\"gs://harmon-arxiv/test_abstracts.json\",\n",
    "                        storage_options={\"token\": \"credentials.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ai</td>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai</td>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai</td>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text\n",
       "0       ai  Because of their occasional need to return to ...\n",
       "1       ai  Market price systems constitute a well-underst...\n",
       "2       ai  We describe an extensive study of search in GS...\n",
       "3       ai  As real logic programmers normally use cut (!)...\n",
       "4       ai  To support the goal of allowing users to recor..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dJT9l0K_ly4"
   },
   "source": [
    "## Vectorizing Text <a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "Machine learning models make use of numerical data in the form of vectors that represent the values of the features in the model. The model is really just an approximation to a function that maps the input vectors to the output (which can be a scalar or vector). In order to build machine learning models that use text data we need convert the text to numerical vectors. To do this we use a [Vector Space Model](https://en.wikipedia.org/wiki/Vector_space_model) which repesents words as vectors in the space.\n",
    "\n",
    "In my [first post](http://michael-harmon.com/blog/NLP1.html) in this series I went over a vector space model called  [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) along with [Term Frequency-Inverse Document Frequency (TF-IDF)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). While these are a classic representations for text, one pifull is that is often produces sparse high dimensional representations. This combination can be particularly challenging for machine learning models to work with. Another shortcoming of the bag-of-words model is that it doesn not take into account order of the words or their semantic relationships. We'll address is the issue of semantic relationships using [Word Embeddings](https://en.wikipedia.org/wiki/Word_embedding), but first we need to discuss how to vectorize text using the TensorFlow [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) class.  \n",
    "\n",
    "First let's grab our data from [Google Cloud Storage](https://cloud.google.com/storage) using [Pandas](https://pandas.pydata.org/) where is stored as json objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the category to numerical values 1, 2, 3, 4 which represent the different abstract topics using the [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "B1jcTPGtM9j_",
    "outputId": "47d7c40e-f7d0-48b5-ecfe-1a2365dd3055"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ai</td>\n",
       "      <td>Because of their occasional need to return to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ai</td>\n",
       "      <td>Market price systems constitute a well-underst...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ai</td>\n",
       "      <td>We describe an extensive study of search in GS...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ai</td>\n",
       "      <td>As real logic programmers normally use cut (!)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ai</td>\n",
       "      <td>To support the goal of allowing users to recor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                               text  target\n",
       "0       ai  Because of their occasional need to return to ...       0\n",
       "1       ai  Market price systems constitute a well-underst...       0\n",
       "2       ai  We describe an extensive study of search in GS...       0\n",
       "3       ai  As real logic programmers normally use cut (!)...       0\n",
       "4       ai  To support the goal of allowing users to recor...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labeler  = LabelEncoder()\n",
    "train_df = train_df.assign(target=labeler.fit_transform(train_df[\"category\"]))\n",
    "val_df   = val_df.assign(target=labeler.fit_transform(val_df[\"category\"]))\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just like with bag-of-words model we need to reperesent documents as vectors in a vector space.  We use the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) class to automatically convert documents to vectors. This class creates a dictionary of words where each word is associated number with an integer index in the dictionary. Words in the sentences are represented by their index value in the vector. The order of the words in the sentence dictate which entry they are in the vector. For example the first word in the sentence has it index value in entry 0 in the vector, the second word has its index value in entry 1 in the vector and so on.\n",
    "\n",
    "We cap the number of words in our dictionary (or vocabulary) at an integer called `max_tokens`. We also set the length of sentences to be capped at `sequence_length` tokens long. That means if our abstract contains less than `sequence_length` words then we pad the rest of the vectors entries with 0's to give it a length of `sequence_length`. If the abstract contains more than `sequence_length` words it will be cut short.\n",
    "\n",
    "Let's first set `sequence_length` by looking at the number of tokens in each document by their category in a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='num_tokens', ylabel='Count'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "train_df = train_df.assign(num_tokens=train_df.text.str.split().apply(len))\n",
    "\n",
    "sns.histplot(data=train_df, x=\"num_tokens\", hue=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most documents are between 50 and 300 words long and the max being close to 400. So let's set our max sequence length to be 400. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll import Tensorflow as well as TextVectorization function. Note the version of TensorFlow I am using is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your verion of Tensorflow, [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) (along with other functions I use) may no longer be experimental and the path may be different.\n",
    "\n",
    "We create a `TextVectorization` layer that has a max sequence length of 400 and cap the vocabulary to be 20,000. Words that are not in this vocabulary will be set to a default \"unknown\" token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6CXq1TOyUEtm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 20:38:23.282845: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-13 20:38:23.282954: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "max_features = 20000\n",
    "sequence_length = 400\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    max_tokens=max_features,\n",
    "    output_mode=\"int\",\n",
    "    pad_to_max_tokens=True,\n",
    "    output_sequence_length=sequence_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the layer on the entire training dataset,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 20:38:23.313578: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-13 20:38:23.342747: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer.adapt(train_df[\"text\"].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example of a text and its vector representation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Because of their occasional need to return to shallow points in a search\\ntree, existing backtracking methods can sometimes erase meaningful progress\\ntoward solving a search problem. In this paper, we present a method by which\\nbacktrack points can be moved deeper in the search space, thereby avoiding this\\ndifficulty. The technique developed is a variant of dependency-directed\\nbacktracking that uses only polynomial space while still providing useful\\ncontrol information and retaining the completeness guarantees provided by\\nearlier approaches.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_df[\"text\"][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the vector representation,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(400,), dtype=int64, numpy=\n",
       "array([ 348,    3,   80, 9056,  408,    6, 2380,    6, 2802,  244,    7,\n",
       "          4,  106,  345,  124, 2220,   39,   22, 2119, 9690, 1526, 1510,\n",
       "       2481,  316,    4,  106,   33,    7,   12,   24,    9,   65,    4,\n",
       "         29,   18,   20, 5380,  244,   22,   19, 6762, 2778,    7,    2,\n",
       "        106,   98, 1666, 2529,   12, 1307,    2,  147,  241,    8,    4,\n",
       "       1115,    3, 9833, 2220,   11,  303,   95,  568,   98,  123,  619,\n",
       "        993,  396,  205,   53,    5, 3918,    2, 2471,  809,  538,   18,\n",
       "       1609,  127,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.call(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our vectors are of max sequence length (300), not the vocabulary length (20,000). Since the above sentence did not have 300 words in it, we pad vector with 0's to make sure the vectors are all of the same length.\n",
    "\n",
    "The i-th entry in the above vector corresponds to the i-th word in sequence of words in the text. The value of the entry in the vector is the index for that word in our vocabulary.\n",
    "\n",
    "The first entry is 8 which means that the word \"We\" is the 8th entry in our 20,000 vocabulary list. The second entry is 309, which means the word \"develop\" is the 309th entry in our vocabulary list.\n",
    "\n",
    "Let's try a word that most likely isnt on our vocabulary to see what the default token's entry is in our vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(400,), dtype=int64, numpy=\n",
       "array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.call(\"onomatopoeia triglycerides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the word \"onomatopeoia\" and \"triglycerides\" are both represented by a 1 which means the unknown token in our vocabulary is 1. We can get the dictionary of words by using the `.get_vocabulary` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorize_layer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalance In The Data <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found the most natural way to deal with imbalanced data in Keras is using weights, similar to the way I did in the [first blog post](http://michael-harmon.com/blog/NLP1.html) in this series. In other frameworks like [Scikit-Learn](https://scikit-learn.org/stable/) the weights for each class are determined automatically, however, with Kera's I found I neeeded to set them explicitly.\n",
    "\n",
    "Luckily we can use Scikit-Learn's [compute_class_weight](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) function to get estimates for the weights,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyTEXiUFUdNy",
    "outputId": "49847bd3-471b-4c29-d0fd-b40cabd7bc05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights: [0.79084967 0.79084967 0.79084967 4.84      ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import class_weight \n",
    "import numpy as np\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "                                   'balanced',\n",
    "                                    classes= np.unique(train_df[\"target\"]), \n",
    "                                    y=train_df[\"target\"])\n",
    "\n",
    "print(f\"class_weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can convert that into a dictionary for each class and its corresponding weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pkzV-GXnUdQX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_dict: {0: 0.7908496732026143, 1: 0.7908496732026143, 2: 0.7908496732026143, 3: 4.84}\n"
     ]
    }
   ],
   "source": [
    "weights_dict = dict(zip(range(4), class_weights))\n",
    "print(f\"weights_dict: {weights_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pt4N1prZUdSu",
    "outputId": "1fb344be-24a9-424c-eaa3-49e28f13783b"
   },
   "source": [
    "We're cheating a little here because were using the entire dataset for calculating the dataset, but we wont be using cross validation so it's not so bad.\n",
    "\n",
    "Let's take a look at the data again, but this time look at distribution of the target variable with respect to the index in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='index', ylabel='target'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.reset_index().plot('index', 'target', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is ordered by the target variable which can be a problem for deep learning models. Deep learning models use [mini-batch gradient descent](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) which use small batches of the original dataset to train on instead of the entire dataset. If the data is ordered according to the target variable each batch most likely will only have data from one class making the model unable to discern differences between classes.\n",
    "\n",
    "To avoid this situation we want to shuffle the dataset first so that the model will get a sufficient representation of  each class in each batch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "train_df = shuffle(train_df)\n",
    "val_df = shuffle(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the datset is not ordered by the target variable value,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='index', ylabel='target'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(train_df.reset_index(drop=True)\n",
    "               .reset_index()\n",
    "               .plot('index', 'target', kind='scatter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data looks much more evenly distributed across the index of the dataframe!\n",
    "\n",
    "Notice that I didnt have to worry about this in prior blog post's models since Scikit-Learn by default uses statified sampling to create folds in [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Those prior models also train on the entire fold in cross-validation.\n",
    "\n",
    "Lastly before we get started with the building a model we one-hot encode the target classes using the [label_binarize](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.label_binarize.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# classes = [0,1,2,3]\n",
    "classes   = np.sort(train_df[\"target\"].unique())\n",
    "\n",
    "# relabel the test set\n",
    "y_train = label_binarize(train_df[\"target\"],\n",
    "                         classes=classes)\n",
    "\n",
    "y_val = label_binarize(val_df[\"target\"], \n",
    "                       classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results are a 4-dimensional vector,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto building a convolutional neural network using Keras!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building A Convolutional Neural Network With Keras <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "We will use a [Convolution Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) with the architecture shown below,\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/Basic-architecture-of-CNN.png\" alt=\"Trulli\" style=\"width:75%\">\n",
    "<figcaption align = \"center\">\n",
    "    <b>From https://www.researchgate.net/figure/Basic-architecture-of-CNN_fig3_335086346</b>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "This is a pretty classic architecture for text classification that uses two [1D Convolutional Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) with an [ReLU activation function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and [1D MaxPooling Layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D). This is followed by a [Dense layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), [Dropout layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) and a [softmax layer](https://en.wikipedia.org/wiki/Softmax_function) for predicting one of the four classes. The one thing that the above diagram does show is that Input Layer is using an [Embedding layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) followed by a Dropout layer.\n",
    "\n",
    "[Word Emeddings](https://en.wikipedia.org/wiki/Word_embedding) (or embeddings) allows us to represent words in a dense low-dimensional vector space instead of a high-dimensional sparse vector space as with the bag-of-words model. In addition to reducing dimensionality they also allow us to learn semantic relationship between words such as man is to woman as king is to queen.\n",
    "\n",
    "A more detailed look at the convolutional/max pooling block is shown below, \n",
    "\n",
    "<figure>\n",
    "<img src=\"images/1d-convolutional.jpg\" alt=\"Trulli\" style=\"width:75%\">\n",
    "<figcaption align = \"center\">\n",
    "    <b>From https://www.researchgate.net/figure/a-Simple-scheme-of-a-one-dimension-1D-convolutional-operation-b-Full_fig2_334609713</b>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "A convolutional layer is made up filters (or kernels) that have weights that must be learned. The kernel size is the number of weights in each filter and are shown in pink. We take the sum-product of the weights and entries in the input vector and apply an [ReLU function](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/) to form new entries in a the output layer. We slide this filter over all windows of size kernel size in the input layer to fill out entries in the output layer (yellow square). In this slidding mechanism we skip entries in the input space. The number of entries that we skip is called the [stride size](https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/). This process process is called a convolution and results in an output that has less dimensions than input. We can supress this reduction in dimension by [padding](https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/) the input vector with 0's so that the output is the same size as the input. Allowing the output space to be reduced in dimension is called \"valid\" padding and supressing this is called \"same\" padding.\n",
    "\n",
    "The [max pooling layer](https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/) looks at the values in a window of the convolutions output layer and finds the maximum value and makes that the value in the output cell. Another version of the operation is shown below.\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/1D-max-pooling-operation.png\" alt=\"Trulli\" style=\"width:75%\">\n",
    "<figcaption align = \"center\">\n",
    "    <b>From https://www.researchgate.net/figure/1D-max-pooling-operation_fig4_324177888</b>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "The window size is again called the `pool_size` (`pooling_size` as labeled above).  MaxPooling also has the concept of `stride` and `padding` that a convolutional layer does. The point of MaxPooling is to make the repesentation more invariant to transformations, a concept that is more intuitive for me when it comes to computer vision.\n",
    "\n",
    "Lastly, we apply [DropOut](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/) which is a form of regularization that randomly sets the weights of specific neurons to  zero during training time. This is shown below,\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/dropout.jpg\" alt=\"Trulli\" style=\"width:75%\">\n",
    "<figcaption align = \"center\">\n",
    "    <b>From https://wenkangwei.github.io/2020/11/13/DL-DropOut/</b>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Drop out prevents overfittings by making the network not overly dependent on any one neuron in the hidden layers. Usually drop out is not performed on in the input layer, as is reserved for the deepest layers as those are the most likely to have overfitting.\n",
    "\n",
    "Now that we have gone over the basics of the CNN architecture let's get building a model! We can import the necessary modules and see if we are using a GPU for training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Input, Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set the logging to only log errors and test to make sure we have a working GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1 .logging.set_verbosity('ERROR')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we creating an embedding layer. Since we dont have too much data for deep learning we will use [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) by using a pre-trained embedding called [GloVe](https://nlp.stanford.edu/projects/glove/). We can create the emedding using the example from the [Keras site](https://keras.io/examples/nlp/pretrained_word_embeddings/). I wrote the function `create_embedding_matrix` to repeat these steps and we can use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.create_glove import create_embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(path=\"glove_models\", \n",
    "                                           dim=embedding_dim, \n",
    "                                           voc=voc,\n",
    "                                           word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20002, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = embedding_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function that returns a compiled Keras model. The function takes in all the hyper-parameters we want to optimize for over our model so that we can feed in different test values of the parameters to find the optimal configuration. The function has default values so that we can call the function without any parameters passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWkwE6_UUdYt",
    "outputId": "91cf1c7f-eb5f-4f69-a779-16f23a391852"
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    embedding_dim: int=128,\n",
    "    filters: int=8,\n",
    "    kernel_size: int=5,\n",
    "    stride_size: int=3,\n",
    "    conv_padding: str=\"valid\",\n",
    "    pool_padding: str=\"same\",\n",
    "    dropout: float=0.2\n",
    ") -> Sequential:\n",
    "    \n",
    "    model = Sequential([\n",
    "                # Embedding Layer\n",
    "                tf.keras.Input(shape=(1,), \n",
    "                               dtype=tf.string, \n",
    "                               name='text'),\n",
    "                vectorize_layer,\n",
    "                embedding_layer,\n",
    "            \n",
    "                # Convolutional Layers\n",
    "                layers.Conv1D(filters, \n",
    "                              kernel_size, \n",
    "                              padding=conv_padding, \n",
    "                              activation=\"relu\", \n",
    "                              strides=stride_size),\n",
    "        \n",
    "                layers.MaxPooling1D(padding=pool_padding),\n",
    "        \n",
    "                layers.Conv1D(filters, \n",
    "                              kernel_size, \n",
    "                              padding=conv_padding, \n",
    "                              activation=\"relu\", \n",
    "                              strides=stride_size),\n",
    "        \n",
    "                layers.GlobalMaxPool1D(),\n",
    "\n",
    "                # Add a vanilla hidden layer:\n",
    "                layers.Dense(filters, activation=\"relu\"),\n",
    "                layers.Dropout(dropout),\n",
    "\n",
    "                # softmax layer\n",
    "                layers.Dense(4, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    # sceduler\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                            initial_learning_rate=1e-2,\n",
    "                            decay_steps=1000,\n",
    "                            decay_rate=0.2)\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", \n",
    "                  optimizer=tf.optimizers.Adam(learning_rate=lr_schedule), \n",
    "                  metrics=[\"accuracy\", tf.keras.metrics.AUC(name='prc', curve='PR')])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWkwE6_UUdYt",
    "outputId": "91cf1c7f-eb5f-4f69-a779-16f23a391852"
   },
   "source": [
    "We can now build the model and see the number of unknowns in each layer as well as the total number of unknowns using the summary method,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVec  (None, 400)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 400, 100)          2000200   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 132, 8)            4008      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 66, 8)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 21, 8)             328       \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 8)                0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8)                 72        \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,004,644\n",
      "Trainable params: 4,444\n",
      "Non-trainable params: 2,000,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of coefficents in this model is dominated by the word embedding. For this configuration the model has over 2 million coefficents which makes it quite complex. However, since we have used a pre-trained embedding the total number of trainable coefficents is a little over 4,000 which is not very many! This small number of trainable coefficents due to the power of transfer learning and the fact CNN's have sparse connections between layers!\n",
    "\n",
    "Next let's turn to optimize in the hyper parameters using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Optuna <a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "Optuna is powerful automatic hyperparameter tuning library that uses a define-by-run design that makes it elegant and easy to use. I have just started using this powerfil library and have been particularly impressed with the design and felt it was extremely intuitve. The three things to take into account for a optimization run in Optun are *Trial*, *Study*, *Parameter*. These are defined as,\n",
    "\n",
    "- **Trial**: A single call of the objective function\n",
    "\n",
    "- **Study**: An optimization session, which is a set of trials\n",
    "\n",
    "- **Parameter**: A variable whose value is to be optimized, such as `embedding_dim` in the `build_model` function.\n",
    "\n",
    "The first thing we do is to define an **objective**; this is the objective function we want to maximize (or minimize). The objective is a function of a trial which is a process of evaluating an objective function provides interfaces to get the **parameter** suggestion.\n",
    "\n",
    "Notice that each hyperparameter is defiend as a parameter in the `build_model` function and is given a suggested value. The model is then fitted to the dataset and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = build_model(embedding_dim=trial.suggest_categorical(\"embedding_dim\", [64, 128, 256, 512]),\n",
    "                        conv_padding=trial.suggest_categorical(\"conv_padding\", [\"valid\", \"same\"]),\n",
    "                        pool_padding=trial.suggest_categorical(\"pool_padding\", [\"valid\", \"same\"]),\n",
    "                        kernel_size=trial.suggest_categorical(\"kernel_size\", [8, 16, 24, 32]),\n",
    "                        stride_size=trial.suggest_categorical(\"strides\", [1, 2, 3, 5]),\n",
    "                        dropout=trial.suggest_float(\"droput\", 0.3, 0.5))\n",
    "    \n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                         restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(train_df[\"text\"], \n",
    "                        y_train, \n",
    "                        epochs=50, \n",
    "                        batch_size=trial.suggest_categorical(\"batch_size\", [32, 64, 128]), ,\n",
    "                        class_weight=weights_dict, \n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping_cb],\n",
    "                        verbose=True)\n",
    "    \n",
    "    scores = history.history[\"val_accuracy\"]\n",
    "    \n",
    "    # get the last epochs scores\n",
    "    return scores[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a backend storage to keep the results of every trial and reload the trial if necessary. In this set up I will use a local [sqlite](https://www.sqlite.org/index.html) database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = optuna.storages.RDBStorage(\n",
    "    url=\"sqlite:///model_tunning.db\",\n",
    "    engine_kwargs={\"connect_args\": {\"timeout\": 10}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a the study with its name and to `maximize` the objective function. We also set the storage to be the sqlite database created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-13 20:41:27,563]\u001b[0m A new study created in RDB with name: study_one\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"study_one\", \n",
    "                            direction=\"maximize\", \n",
    "                            storage=storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well just create 10 studies to show the effectiveness of Optuna and fead the study the objective function through the `optimize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, \n",
    "               n_trials=5, \n",
    "               timeout=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_conv_padding</th>\n",
       "      <th>params_droput</th>\n",
       "      <th>params_embedding_dim</th>\n",
       "      <th>params_kernel_size</th>\n",
       "      <th>params_pool_padding</th>\n",
       "      <th>params_strides</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.438017</td>\n",
       "      <td>2022-11-13 20:06:07.438234</td>\n",
       "      <td>2022-11-13 20:06:49.010267</td>\n",
       "      <td>0 days 00:00:41.572033</td>\n",
       "      <td>64</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>256</td>\n",
       "      <td>24</td>\n",
       "      <td>valid</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.760331</td>\n",
       "      <td>2022-11-13 20:06:49.032452</td>\n",
       "      <td>2022-11-13 20:07:45.814676</td>\n",
       "      <td>0 days 00:00:56.782224</td>\n",
       "      <td>64</td>\n",
       "      <td>same</td>\n",
       "      <td>0.439536</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.689050</td>\n",
       "      <td>2022-11-13 20:07:45.834451</td>\n",
       "      <td>2022-11-13 20:08:03.545316</td>\n",
       "      <td>0 days 00:00:17.710865</td>\n",
       "      <td>128</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.439098</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>same</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.768595</td>\n",
       "      <td>2022-11-13 20:08:03.556305</td>\n",
       "      <td>2022-11-13 20:08:47.528046</td>\n",
       "      <td>0 days 00:00:43.971741</td>\n",
       "      <td>64</td>\n",
       "      <td>same</td>\n",
       "      <td>0.348178</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.586777</td>\n",
       "      <td>2022-11-13 20:08:47.538103</td>\n",
       "      <td>2022-11-13 20:09:13.199656</td>\n",
       "      <td>0 days 00:00:25.661553</td>\n",
       "      <td>128</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.391928</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>valid</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>2022-11-13 20:09:13.216166</td>\n",
       "      <td>2022-11-13 20:09:44.558993</td>\n",
       "      <td>0 days 00:00:31.342827</td>\n",
       "      <td>64</td>\n",
       "      <td>same</td>\n",
       "      <td>0.425492</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>same</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.604339</td>\n",
       "      <td>2022-11-13 20:09:44.572661</td>\n",
       "      <td>2022-11-13 20:10:15.148391</td>\n",
       "      <td>0 days 00:00:30.575730</td>\n",
       "      <td>64</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.328567</td>\n",
       "      <td>128</td>\n",
       "      <td>24</td>\n",
       "      <td>valid</td>\n",
       "      <td>3</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.806818</td>\n",
       "      <td>2022-11-13 20:10:15.160535</td>\n",
       "      <td>2022-11-13 20:10:53.946238</td>\n",
       "      <td>0 days 00:00:38.785703</td>\n",
       "      <td>128</td>\n",
       "      <td>same</td>\n",
       "      <td>0.486128</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>same</td>\n",
       "      <td>2</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.846074</td>\n",
       "      <td>2022-11-13 20:10:53.959711</td>\n",
       "      <td>2022-11-13 20:11:26.273024</td>\n",
       "      <td>0 days 00:00:32.313313</td>\n",
       "      <td>64</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.494125</td>\n",
       "      <td>128</td>\n",
       "      <td>24</td>\n",
       "      <td>same</td>\n",
       "      <td>5</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.799587</td>\n",
       "      <td>2022-11-13 20:11:26.285632</td>\n",
       "      <td>2022-11-13 20:11:52.998722</td>\n",
       "      <td>0 days 00:00:26.713090</td>\n",
       "      <td>128</td>\n",
       "      <td>valid</td>\n",
       "      <td>0.320053</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>same</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value             datetime_start          datetime_complete  \\\n",
       "0       0  0.438017 2022-11-13 20:06:07.438234 2022-11-13 20:06:49.010267   \n",
       "1       1  0.760331 2022-11-13 20:06:49.032452 2022-11-13 20:07:45.814676   \n",
       "2       2  0.689050 2022-11-13 20:07:45.834451 2022-11-13 20:08:03.545316   \n",
       "3       3  0.768595 2022-11-13 20:08:03.556305 2022-11-13 20:08:47.528046   \n",
       "4       4  0.586777 2022-11-13 20:08:47.538103 2022-11-13 20:09:13.199656   \n",
       "5       5  0.840909 2022-11-13 20:09:13.216166 2022-11-13 20:09:44.558993   \n",
       "6       6  0.604339 2022-11-13 20:09:44.572661 2022-11-13 20:10:15.148391   \n",
       "7       7  0.806818 2022-11-13 20:10:15.160535 2022-11-13 20:10:53.946238   \n",
       "8       8  0.846074 2022-11-13 20:10:53.959711 2022-11-13 20:11:26.273024   \n",
       "9       9  0.799587 2022-11-13 20:11:26.285632 2022-11-13 20:11:52.998722   \n",
       "\n",
       "                duration  params_batch_size params_conv_padding  \\\n",
       "0 0 days 00:00:41.572033                 64               valid   \n",
       "1 0 days 00:00:56.782224                 64                same   \n",
       "2 0 days 00:00:17.710865                128               valid   \n",
       "3 0 days 00:00:43.971741                 64                same   \n",
       "4 0 days 00:00:25.661553                128               valid   \n",
       "5 0 days 00:00:31.342827                 64                same   \n",
       "6 0 days 00:00:30.575730                 64               valid   \n",
       "7 0 days 00:00:38.785703                128                same   \n",
       "8 0 days 00:00:32.313313                 64               valid   \n",
       "9 0 days 00:00:26.713090                128               valid   \n",
       "\n",
       "   params_droput  params_embedding_dim  params_kernel_size  \\\n",
       "0       0.302682                   256                  24   \n",
       "1       0.439536                   128                  32   \n",
       "2       0.439098                    64                   8   \n",
       "3       0.348178                   128                  32   \n",
       "4       0.391928                    64                  32   \n",
       "5       0.425492                    64                   8   \n",
       "6       0.328567                   128                  24   \n",
       "7       0.486128                   256                  32   \n",
       "8       0.494125                   128                  24   \n",
       "9       0.320053                   128                   8   \n",
       "\n",
       "  params_pool_padding  params_strides     state  \n",
       "0               valid               3  COMPLETE  \n",
       "1               valid               1  COMPLETE  \n",
       "2                same               3  COMPLETE  \n",
       "3               valid               1  COMPLETE  \n",
       "4               valid               1  COMPLETE  \n",
       "5                same               5  COMPLETE  \n",
       "6               valid               3  COMPLETE  \n",
       "7                same               2  COMPLETE  \n",
       "8                same               5  COMPLETE  \n",
       "9                same               1  COMPLETE  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8574380874633789"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'conv_padding': 'valid',\n",
       " 'droput': 0.3392705552866529,\n",
       " 'embedding_dim': 128,\n",
       " 'kernel_size': 24,\n",
       " 'pool_padding': 'same',\n",
       " 'strides': 5}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPWTTd7yPkxh2nn2xIX0vc4",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP-Part4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (deepgreen)",
   "language": "python",
   "name": "deepgreen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
